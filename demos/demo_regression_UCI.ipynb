{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.OutputArea.auto_scroll_threshold = 9999;"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "IPython.OutputArea.auto_scroll_threshold = 9999;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DGP for regression\n",
    "\n",
    "Here we'll show the DGP for regression, using small to medium data sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hughsalimbeni/anaconda3/envs/prowler_env/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.5\n",
      "  return f(*args, **kwds)\n",
      "/Users/hughsalimbeni/anaconda3/envs/prowler_env/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "/Users/hughsalimbeni/anaconda3/envs/prowler_env/lib/python3.5/site-packages/multipledispatch/dispatcher.py:24: AmbiguityWarning: \n",
      "Ambiguities exist in dispatched function _expectation\n",
      "\n",
      "The following signatures may result in ambiguous behavior:\n",
      "\t[Gaussian, Identity, NoneType, Kernel, InducingPoints], [Gaussian, Linear, NoneType, Sum, InducingPoints]\n",
      "\n",
      "\n",
      "Consider making the following additions:\n",
      "\n",
      "@dispatch(Gaussian, Identity, NoneType, Sum, InducingPoints)\n",
      "def _expectation(...)\n",
      "  warn(warning_text(dispatcher.name, ambiguities), AmbiguityWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "tf.logging.set_verbosity(0)\n",
    "\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "from gpflow.likelihoods import Gaussian\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.mean_functions import Constant\n",
    "from gpflow.models.sgpr import SGPR, GPRFITC\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.models.gpr import GPR\n",
    "from gpflow.training import AdamOptimizer, ScipyOptimizer, NatGradOptimizer\n",
    "from gpflow.actions import Action, Loop\n",
    "\n",
    "from scipy.cluster.vq import kmeans2\n",
    "from scipy.stats import norm\n",
    "from scipy.special import logsumexp\n",
    "\n",
    "from doubly_stochastic_dgp.dgp import DGP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "import csv\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, name, N, D, type, data_path='/data/'):\n",
    "        self.data_path = data_path\n",
    "        self.name, self.N, self.D = name, N, D\n",
    "        assert type in ['regression', 'classification', 'multiclass']\n",
    "        self.type = type\n",
    "\n",
    "    def csv_file_path(self, name):\n",
    "        return '{}{}.csv'.format(self.data_path, name)\n",
    "\n",
    "    def read_data(self):\n",
    "        data = pandas.read_csv(self.csv_file_path(self.name),\n",
    "                               header=None, delimiter=',').values\n",
    "        return {'X':data[:, :-1], 'Y':data[:, -1, None]}\n",
    "\n",
    "    def download_data(self):\n",
    "        NotImplementedError\n",
    "\n",
    "    def get_data(self, seed=0, split=0, prop=0.9):\n",
    "        path = self.csv_file_path(self.name)\n",
    "        if not os.path.isfile(path):\n",
    "            self.download_data()\n",
    "\n",
    "        full_data = self.read_data()\n",
    "        split_data = self.split(full_data, seed, split, prop)\n",
    "        split_data = self.normalize(split_data, 'X')\n",
    "\n",
    "        if self.type is 'regression':\n",
    "            split_data = self.normalize(split_data, 'Y')\n",
    "\n",
    "        return split_data\n",
    "\n",
    "    def split(self, full_data, seed, split, prop):\n",
    "        ind = np.arange(self.N)\n",
    "\n",
    "        np.random.seed(seed + split)\n",
    "        np.random.shuffle(ind)\n",
    "\n",
    "        n = int(self.N * prop)\n",
    "\n",
    "        X = full_data['X'][ind[:n], :]\n",
    "        Xs = full_data['X'][ind[n:], :]\n",
    "\n",
    "        Y = full_data['Y'][ind[:n], :]\n",
    "        Ys = full_data['Y'][ind[n:], :]\n",
    "\n",
    "        return {'X': X, 'Xs': Xs, 'Y': Y, 'Ys': Ys}\n",
    "\n",
    "    def normalize(self, split_data, X_or_Y):\n",
    "        m = np.average(split_data[X_or_Y], 0)[None, :]\n",
    "        s = np.std(split_data[X_or_Y + 's'], 0)[None, :] + 1e-6\n",
    "\n",
    "        split_data[X_or_Y] = (split_data[X_or_Y] - m) / s\n",
    "        split_data[X_or_Y + 's'] = (split_data[X_or_Y + 's'] - m) / s\n",
    "\n",
    "        split_data.update({X_or_Y + '_mean': m.flatten()})\n",
    "        split_data.update({X_or_Y + '_std': s.flatten()})\n",
    "        return split_data\n",
    "\n",
    "\n",
    "class Boston(Dataset):\n",
    "    def __init__(self):\n",
    "        self.name, self.N, self.D = 'boston', 506, 12\n",
    "        self.type = 'regression'\n",
    "\n",
    "    def download_data(self):\n",
    "        url = '{}{}'.format(uci_base, 'housing/housing.data')\n",
    "\n",
    "        data = pandas.read_fwf(url, header=None).values\n",
    "        with open(self.csv_file_path(self.name), 'w') as f:\n",
    "            csv.writer(f).writerows(data)\n",
    "            \n",
    "bost = Dataset(\"housing\", 506, 12, \"regression\", \"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from scipy.io import loadmat\n",
    "\n",
    "from urllib.request import urlopen\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "import zipfile\n",
    "\n",
    "BASE_SEED = 123\n",
    "DATA_PATH = \"data/\"\n",
    "\n",
    "_ALL_REGRESSION_DATATSETS = {}\n",
    "_ALL_CLASSIFICATION_DATATSETS = {}\n",
    "\n",
    "def add_regression(C):\n",
    "    _ALL_REGRESSION_DATATSETS.update({C.name:C})\n",
    "    return C\n",
    "\n",
    "def add_classficiation(C):\n",
    "    _ALL_CLASSIFICATION_DATATSETS.update({C.name:C})\n",
    "    return C\n",
    "\n",
    "def normalize(X):\n",
    "    X_mean = np.average(X, 0)[None, :]\n",
    "    X_std = 1e-6 + np.std(X, 0)[None, :]\n",
    "    return (X - X_mean) / X_std, X_mean, X_std\n",
    "\n",
    "\n",
    "class Dataset(object):\n",
    "    def __init__(self, split=0, prop=0.9):\n",
    "        if self.needs_download:\n",
    "            self.download()\n",
    "\n",
    "        X_raw, Y_raw = self.read_data()\n",
    "        X, Y = self.preprocess_data(X_raw, Y_raw)\n",
    "\n",
    "        ind = np.arange(self.N)\n",
    "\n",
    "        np.random.seed(BASE_SEED + split)\n",
    "        np.random.shuffle(ind)\n",
    "\n",
    "        n = int(self.N * prop)\n",
    "\n",
    "        self.X_train = X[ind[:n]]\n",
    "        self.Y_train = Y[ind[:n]]\n",
    "\n",
    "        self.X_test = X[ind[n:]]\n",
    "        self.Y_test = Y[ind[n:]]\n",
    "\n",
    "    @property\n",
    "    def datadir(self):\n",
    "        dir = os.path.join(DATA_PATH, self.name)\n",
    "        if not os.path.isdir(dir):\n",
    "            os.mkdir(dir)\n",
    "        return dir\n",
    "\n",
    "    @property\n",
    "    def datapath(self):\n",
    "        filename = self.url.split('/')[-1]  # this is for the simple case with no zipped files\n",
    "        return os.path.join(self.datadir, filename)\n",
    "\n",
    "    @property\n",
    "    def needs_download(self):\n",
    "        return not os.path.isfile(self.datapath)\n",
    "\n",
    "    def download(self):\n",
    "        logging.info('donwloading {} data'.format(self.name))\n",
    "\n",
    "        is_zipped = np.any([z in self.url for z in ['.gz', '.zip', '.tar']])\n",
    "\n",
    "        if is_zipped:\n",
    "            filename = os.path.join(self.datadir, self.url.split('/')[-1])\n",
    "        else:\n",
    "            filename = self.datapath\n",
    "\n",
    "        with urlopen(self.url) as response, open(filename, 'wb') as out_file:\n",
    "            data = response.read()\n",
    "            out_file.write(data)\n",
    "\n",
    "        if is_zipped:\n",
    "            zip_ref = zipfile.ZipFile(filename, 'r')\n",
    "            zip_ref.extractall(self.datadir)\n",
    "            zip_ref.close()\n",
    "\n",
    "            # os.remove(filename)\n",
    "\n",
    "        logging.info('finished donwloading {} data'.format(self.name))\n",
    "\n",
    "    def read_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def preprocess_data(self, X, Y):\n",
    "        X, self.X_mean, self.X_std = normalize(X)\n",
    "        Y, self.Y_mean, self.Y_std = normalize(Y)\n",
    "        return X, Y\n",
    "\n",
    "\n",
    "uci_base_url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/'\n",
    "\n",
    "\n",
    "class Boston(Dataset):\n",
    "    N, D, name = 506, 13, 'boston'\n",
    "    url = uci_base_url + 'housing/housing.data'\n",
    "\n",
    "    def read_data(self):\n",
    "        data = pandas.read_fwf(self.datapath, header=None).values\n",
    "        return data[:, :-1], data[:, -1].reshape(-1, 1)\n",
    "    \n",
    "    \n",
    "class Datasets(object):\n",
    "    def __init__(self, data_path='/data/'):\n",
    "        if not os.path.isdir(data_path):\n",
    "            os.mkdir(data_path)\n",
    "\n",
    "        datasets = []\n",
    "        datasets.append(Boston())\n",
    "        self.all_datasets = {}\n",
    "        for d in datasets:\n",
    "            d.data_path = data_path\n",
    "            self.all_datasets.update({d.name : d})\n",
    "\n",
    "            \n",
    "datasets = Datasets(data_path='data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.18801255]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets.all_datasets['boston'].Y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.21897961, -0.48772234,  1.01599892, ...,  0.80657546,\n",
       "        -3.88219453, -0.35647075],\n",
       "       [-0.6258056 , -0.48772234, -0.96982698, ...,  0.02055959,\n",
       "         0.39061588, -0.85829282],\n",
       "       [-0.63394249,  0.37066898, -1.1390818 , ..., -1.64394462,\n",
       "         0.33590373, -1.24657415],\n",
       "       ...,\n",
       "       [-0.63422919,  2.94584296, -0.90270869, ...,  0.34421318,\n",
       "         0.44105193, -1.30684886],\n",
       "       [ 0.88061431, -0.48772234,  1.01599892, ...,  0.80657546,\n",
       "        -3.5264011 ,  1.20085977],\n",
       "       [-0.62400997,  1.22906031, -1.44257319, ...,  0.57539432,\n",
       "         0.44105193, -0.93538839]])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N: 7372, D: 8, Ns: 820\n"
     ]
    }
   ],
   "source": [
    "data = datasets.all_datasets['boston'].get_data()\n",
    "X, Y, Xs, Ys, Y_std = [data[_] for _ in ['X', 'Y', 'Xs', 'Ys', 'Y_std']]\n",
    "print('N: {}, D: {}, Ns: {}'.format(X.shape[0], X.shape[1], Xs.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer models\n",
    "\n",
    "Our baseline model is a sparse GP, but since the dataset is small we can also train without minibatches so we'll also compare to a collapsed sparse GP (with analytically optimal $q(\\mathbf u)$) which is known as SGPR in GPflow terminology, and we'll also cpmpare to FITC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_single_layer_models(X, Y, Z):\n",
    "    D = X.shape[1]\n",
    "    m_sgpr = SGPR(X, Y, RBF(D), Z.copy())\n",
    "    m_svgp = SVGP(X, Y, RBF(D), Gaussian(), Z.copy())\n",
    "    m_fitc = GPRFITC(X, Y, RBF(D), Z.copy())\n",
    "    for m in m_sgpr, m_svgp, m_fitc:\n",
    "        m.likelihood.variance = 0.01\n",
    "    return [m_sgpr, m_svgp, m_fitc], ['{} {}'.format(n, len(Z)) for n in ['SGPR', 'SVGP', 'FITC']]\n",
    "\n",
    "Z_100 = kmeans2(X, 100, minit='points')[0]\n",
    "models_single_layer, names_single_layer = make_single_layer_models(X, Y, Z_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DGP models\n",
    "\n",
    "We'll include a DGP with a single layer here for comparision. We've used a largish minibatch size of $\\text{min}(1000, N)$, but it works fine for smaller batches too\n",
    "\n",
    "In the paper we used 1 sample. Here we'll go up to 5 in celebration of the new implementation (which is much more efficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dgp_models(X, Y, Z):\n",
    "    models, names = [], []\n",
    "    for L in range(1, 4):\n",
    "        D = X.shape[1]\n",
    "\n",
    "        # the layer shapes are defined by the kernel dims, so here all hidden layers are D dimensional \n",
    "        kernels = []\n",
    "        for l in range(L):\n",
    "            kernels.append(RBF(D))\n",
    "\n",
    "        # between layer noise (doesn't actually make much difference but we include it anyway)\n",
    "        for kernel in kernels[:-1]:\n",
    "            kernel += White(D, variance=1e-5) \n",
    "\n",
    "        mb = 1000 if X.shape[0] > 1000 else None \n",
    "        model = DGP(X, Y, Z, kernels, Gaussian(), num_samples=5, minibatch_size=mb)\n",
    "\n",
    "        # start the inner layers almost deterministically \n",
    "        for layer in model.layers[:-1]:\n",
    "            layer.q_sqrt = layer.q_sqrt.value * 1e-5\n",
    "\n",
    "        models.append(model)\n",
    "        names.append('DGP{} {}'.format(L, len(Z)))\n",
    "    \n",
    "    return models, names\n",
    "\n",
    "models_dgp, names_dgp = make_dgp_models(X, Y, Z_100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction\n",
    "\n",
    "We'll calculate test rmse and likelihood in batches (so the larger datasets don't cause memory problems)\n",
    "\n",
    "For the DGP models we need to take an average over the samples for the rmse. The `predict_density` function already does this internally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(X.shape[0]/1000.), 1)\n",
    "    lik, sq_diff = [], []\n",
    "    for X_batch, Y_batch in zip(np.array_split(X, n_batches), np.array_split(Y, n_batches)):\n",
    "        l, sq = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        sq_diff.append(sq)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    sq_diff = np.array(np.concatenate(sq_diff, 0), dtype=float)\n",
    "    return np.average(lik), np.average(sq_diff)**0.5\n",
    "\n",
    "def assess_single_layer(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch)\n",
    "    lik = np.sum(norm.logpdf(Y_batch*Y_std, loc=m*Y_std, scale=Y_std*v**0.5),  1)\n",
    "    sq_diff = Y_std**2*((m - Y_batch)**2)\n",
    "    return lik, sq_diff \n",
    "\n",
    "S = 100\n",
    "def assess_sampled(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch, S)\n",
    "    S_lik = np.sum(norm.logpdf(Y_batch*Y_std, loc=m*Y_std, scale=Y_std*v**0.5), 2)\n",
    "    lik = logsumexp(S_lik, 0, b=1/float(S))\n",
    "    \n",
    "    mean = np.average(m, 0)\n",
    "    sq_diff = Y_std**2*((mean - Y_batch)**2)\n",
    "    return lik, sq_diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training \n",
    "\n",
    "We'll optimize single layer models and using LFBGS and the dgp models with Adam. It will be interesting to compare the result of `m_svgp` compared to `m_dgp1`: if there is a difference it will be down to the optimizer. \n",
    "\n",
    "We'll show here also the reuslt of using a small and large number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations_few = 100\n",
    "iterations_many = 5000\n",
    "s = '{:<16}  lik: {:.4f}, rmse: {:.4f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 100 iterations\n",
      "SGPR 100          lik: 0.9481, rmse: 0.0895\n",
      "SVGP 100          lik: 0.7620, rmse: 0.1090\n",
      "FITC 100          lik: 1.0514, rmse: 0.0910\n",
      "after 5000 iterations\n",
      "SGPR 100          lik: 0.9758, rmse: 0.0864\n",
      "SVGP 100          lik: 0.9736, rmse: 0.0868\n",
      "FITC 100          lik: 1.1284, rmse: 0.0828\n"
     ]
    }
   ],
   "source": [
    "for iterations in [iterations_few, iterations_many]:\n",
    "    print('after {} iterations'.format(iterations))\n",
    "    for m, name in zip(models_single_layer, names_single_layer):\n",
    "        ScipyOptimizer().minimize(m, maxiter=iterations)\n",
    "        lik, rmse = batch_assess(m, assess_single_layer, Xs, Ys)\n",
    "        print(s.format(name, lik, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the DGP models. First we use Adam for all parameters (as in the Doubly Stochastic VI for DGPs paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 100 iterations\n",
      "DGP1 100          lik: 0.2778, rmse: 0.1139\n",
      "DGP2 100          lik: 0.2394, rmse: 0.1170\n",
      "DGP3 100          lik: 0.2165, rmse: 0.1289\n",
      "after 5000 iterations\n",
      "DGP1 100          lik: 0.9434, rmse: 0.0896\n",
      "DGP2 100          lik: 1.2913, rmse: 0.0661\n",
      "DGP3 100          lik: 1.3039, rmse: 0.0655\n"
     ]
    }
   ],
   "source": [
    "for iterations in [iterations_few, iterations_many]:\n",
    "    print('after {} iterations'.format(iterations))\n",
    "    for m, name in zip(models_dgp, names_dgp):\n",
    "        AdamOptimizer(0.01).minimize(m, maxiter=iterations)\n",
    "        lik, rmse = batch_assess(m, assess_sampled, Xs, Ys)\n",
    "        print(s.format(name, lik, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use natural gradients for the final layer, which can help considerably. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after 100 iterations\n",
      "DGP1 100          lik: 0.9487, rmse: 0.0891\n",
      "DGP2 100          lik: 1.2837, rmse: 0.0668\n",
      "DGP3 100          lik: 1.2958, rmse: 0.0661\n",
      "after 5000 iterations\n",
      "DGP1 100          lik: 0.9549, rmse: 0.0890\n",
      "DGP2 100          lik: 1.2915, rmse: 0.0664\n",
      "DGP3 100          lik: 1.3147, rmse: 0.0650\n"
     ]
    }
   ],
   "source": [
    "for iterations in [iterations_few, iterations_many]:\n",
    "    print('after {} iterations'.format(iterations))\n",
    "    for m, name in zip(models_dgp, names_dgp):\n",
    "        ng_vars = [[m.layers[-1].q_mu, m.layers[-1].q_sqrt]]\n",
    "        for v in ng_vars[0]:\n",
    "            v.set_trainable(False)    \n",
    "        ng_action = NatGradOptimizer(gamma=0.1).make_optimize_action(m, var_list=ng_vars)\n",
    "        adam_action = AdamOptimizer(0.01).make_optimize_action(m)\n",
    "\n",
    "        Loop([ng_action, adam_action], stop=iterations)()\n",
    "\n",
    "        lik, rmse = batch_assess(m, assess_sampled, Xs, Ys)\n",
    "        print(s.format(name, lik, rmse))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even after 100 iterations we get a good result, which is not the case using ordinary gradients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
