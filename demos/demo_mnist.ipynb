{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /data/MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting /data/MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting /data/MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /data/MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from gpflow.likelihoods import MultiClass\n",
    "from gpflow.kernels import RBF, White\n",
    "from gpflow.models.svgp import SVGP\n",
    "from gpflow.training import AdamOptimizer\n",
    "\n",
    "from scipy.stats import mode\n",
    "from scipy.cluster.vq import kmeans2\n",
    "\n",
    "from doubly_stochastic_dgp.dgp import DGP\n",
    "\n",
    "import time\n",
    "\n",
    "def get_mnist_data(data_path='/data'):\n",
    "    from tensorflow.examples.tutorials.mnist import input_data\n",
    "    mnist = input_data.read_data_sets(data_path+'/MNIST_data/', one_hot=False)\n",
    "\n",
    "    X, Y = mnist.train.next_batch(mnist.train.num_examples)\n",
    "    Xval, Yval = mnist.validation.next_batch(mnist.validation.num_examples)\n",
    "    Xtest, Ytest = mnist.test.next_batch(mnist.test.num_examples)\n",
    "\n",
    "    Y, Yval, Ytest = [np.array(y, dtype=float)[:, None] for y in [Y, Yval, Ytest]]\n",
    "\n",
    "    X = np.concatenate([X, Xval], 0)\n",
    "    Y = np.concatenate([Y, Yval], 0)\n",
    "    \n",
    "    return X.astype(float), Y.astype(float), Xtest.astype(float), Ytest.astype(float)\n",
    "\n",
    "X, Y, Xs, Ys = get_mnist_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use 100 inducing points "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = 100\n",
    "Z = kmeans2(X, M, minit='points')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll compare three models: an ordinary sparse GP and DGPs with 2 and 3 layers. \n",
    "\n",
    "We'll use a batch size of 1000 for all models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_sgp = SVGP(X, Y, RBF(784, lengthscales=2., variance=2.), MultiClass(10), \n",
    "             Z=Z, num_latent=10, minibatch_size=1000, whiten=True)\n",
    "\n",
    "def make_dgp(L):\n",
    "    kernels = [RBF(784, lengthscales=2., variance=2.)]\n",
    "    for l in range(L-1):\n",
    "        kernels.append(RBF(30, lengthscales=2., variance=2.))\n",
    "    model = DGP(X, Y, Z, kernels, MultiClass(10), \n",
    "                minibatch_size=1000,\n",
    "                num_outputs=10)\n",
    "    \n",
    "    # start things deterministic \n",
    "    for layer in model.layers[:-1]:\n",
    "        layer.q_sqrt = layer.q_sqrt.value * 1e-5 \n",
    "    \n",
    "    return model\n",
    "\n",
    "m_dgp2 = make_dgp(2)\n",
    "m_dgp3 = make_dgp(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the SGP model we'll calcuate accuracy by simply taking the max mean prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model_sgp(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch)\n",
    "    l = model.predict_density(X_batch, Y_batch)\n",
    "    a = (np.argmax(m, 1).reshape(Y_batch.shape).astype(int)==Y_batch.astype(int))\n",
    "    return l, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the DGP models we have stochastic predictions. We need a single prediction for each datum, so to do this we take $S$ samples for the one-hot predictions ($(S, N, 10)$ matrices for mean and var), then we take the max over the class means (to give a $(S, N)$ matrix), and finally we take the modal class over the samples (to give a vector of length $N$):\n",
    "\n",
    "We'll use 100 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = 100\n",
    "def assess_model_dgp(model, X_batch, Y_batch):\n",
    "    m, v = model.predict_y(X_batch, S)\n",
    "    l = model.predict_density(X_batch, Y_batch, S)\n",
    "    a = (mode(np.argmax(m, 2), 0)[0].reshape(Y_batch.shape).astype(int)==Y_batch.astype(int))\n",
    "    return l, a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need batch predictions (we might run out of memory otherwise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_assess(model, assess_model, X, Y):\n",
    "    n_batches = max(int(len(X)/1000), 1)\n",
    "    lik, acc = [], []\n",
    "    for X_batch, Y_batch in zip(np.split(X, n_batches), np.split(Y, n_batches)):\n",
    "        l, a = assess_model(model, X_batch, Y_batch)\n",
    "        lik.append(l)\n",
    "        acc.append(a)\n",
    "    lik = np.concatenate(lik, 0)\n",
    "    acc = np.array(np.concatenate(acc, 0), dtype=float)\n",
    "    return np.average(lik), np.average(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to go\n",
    "\n",
    "The sparse GP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sgp test lik: -0.1092, test acc 0.9698\n"
     ]
    }
   ],
   "source": [
    "AdamOptimizer(0.01).minimize(m_sgp, maxiter=iterations)\n",
    "l, a = batch_assess(m_sgp, assess_model_sgp, Xs, Ys)\n",
    "print('sgp test lik: {:.4f}, test acc {:.4f}'.format(l, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using more inducing points improves things, but at the expense of very slow computation (500 inducing points takes about a day)\n",
    "\n",
    "The two layer DGP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgp2 test lik: -0.0731, test acc 0.9794\n"
     ]
    }
   ],
   "source": [
    "AdamOptimizer(0.01).minimize(m_dgp2, maxiter=iterations)\n",
    "l, a = batch_assess(m_dgp2, assess_model_dgp, Xs, Ys)\n",
    "print('dgp2 test lik: {:.4f}, test acc {:.4f}'.format(l, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the three layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dgp3 test lik: -0.0709, test acc 0.9799\n"
     ]
    }
   ],
   "source": [
    "AdamOptimizer(0.01).minimize(m_dgp3, maxiter=iterations)\n",
    "l, a = batch_assess(m_dgp3, assess_model_dgp, Xs, Ys)\n",
    "print('dgp3 test lik: {:.4f}, test acc {:.4f}'.format(l, a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the deeper models we see a small improvement in accuracy, and a larger improvement in test log likelihood "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
